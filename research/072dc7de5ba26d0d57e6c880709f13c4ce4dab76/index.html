<html>
    <head>
        <link rel="stylesheet" href="/css/research.css">
        
        <script type="text/javascript" src="/js/latexit.js"></script>
        <script type="text/javascript">
            LatexIT.add('p',true);
        </script>
        
        
        <script src="/js/d3.v4.min.js"></script>
        
        <title>Weekly meeting</title>
    </head>
    <body>
        
        <h1 style="text-align: center">Deep ModelNets with Plug & Play Geometric Vision Modules</h1>
        <h2 style="text-align: center">Mohit Rathore</h1>
        <!-- <svg class="bar-chart"></svg> -->
        
        <a href="#" class="more btn" id="abstract">ABSTRACT</a>
        <div class='abstract' style='text-align: left; left:0; right:0 ;margin-left:300; margin-right:300;'>
            <p>With the advent of neural-network based object tracking techniques, geometrical approaches for object tracking have lost their grandeur. Though these neural-network based techniques suffer from an inherent problem of relying too much
                on training data. These technniques go hay-wire in unknown environments where the geometrical approaches still prevail. This can be attributed to their
                model agnostic behaviour. In this work we introduce a technique of coalescing these two approaches such that they overcome each other's drawbacks by using
                deep modelnets with pluggable geometric vision modules. We show that this technique is more versatile and robust then the existing object tracking techniques.
            </p>
        </div>        

        <a href="#" class="more btn" id="intro">1. INTRODUCTION</a>
        <div class='intro' style='text-align: center; left:0; right:0 ;margin-left:80; margin-right:80;'>
            <p>like <a href="#">KLT</a>,<a href="#">SFM/SLAM</a>,
                depth/pose/EgoMotion estimation
            </p>

            <!-- <p>The general idea of object tracking is followed as - </p>
            <ul>
                <li>In the first frame, we mark an set of points in which our object lies. These points bound the object we wish to track.</li>
                <li>In subsequent frames it is expected that we will keep getting a new set of coordinates which will keep bounding the object.</li>
            </ul>
            <div style="width:image width px; font-size:90%; text-align:center">
                    <img src = "/img/research/frame1.png" alt = 'alt text'/>
                    <br/>
                    Frame marking the object to be tracked.
                    Figure 1
                </div> -->
        </div>
           
        <a href="#" class="more btn" id="work">2. RELATED WORK</a>
        <div class='work' style='text-align: center; left:0; right:0 ;margin-left:80; margin-right:80;'>
            <p>like <a href="#">KLT</a>,<a href="#">SFM/SLAM</a>,
                depth/pose/EgoMotion estimation
            </p>

        </div>

        <a href="#" class="more btn" id="our">3. DEEP MODELNETS WITH PLUG & PLAY GEOMETRIC VISION MODULES</a>
        <div class='our' style='left:0; right:0 ;margin-left:30; margin-right:50;'>
            <p>With the advent of deep learning we can use neural networks to our aid.</p>
            <ul>
                <li>
                    We can use neural networks at appropriate position in KLT, possibly right after calculating the gradient of the image so as to extract important features
                    (which then act as a proxy for image gradients in KLT) that can account for the reliablity of the gradient and make the system more robust.
                </li>
                <li>
                    There are techniques already present which make use of the various flavour of neural networks for object tracking, but these techniques are known to
                    fail at instances because they go haywire in unknown environments (the environments which were not present in the training data.) 
                </li>
                <li>
                    Our proposed solution aims to capture the best of both worlds making a more reliable and robust object tracking system.
                </li>
            </ul>
            <div style="width:image width px; font-size:90%; text-align:center">
                <img src = "/img/research/neural.jpg" style="width:40%; height:60%;"  alt = 'alt text'/>
                <br/>
            </div>    
        </div>
        <!-- http://www.eng.tau.ac.il/~oron/TWD/TWD.html -->
        <a href="#" class="more btn" id="exp">4. EXPERIMENTS</a>
        <div class='exp' style='text-align: center; left:0; right:0 ;margin-left:80; margin-right:80;'>
            <p> We test our framework on a number of datasets. In section 4.1 we apply our technique on Vehicle View Point Dataset. 
            </p>
            <h2>4.1 Vehicle View Point Dataset</h2>
            <p>The dataset has three different view points - RearCenter, PassengerSide and DriverSide in 166 scenarios with a total of 35414 frames.
               The dataset is completely annotated.</p>
            
        </div>

        <a href="#" class="more btn" id="conc">5. CONCLUSION</a>
        <div class='conc' style='text-align: center; left:0; right:0 ;margin-left:80; margin-right:80;'>
            <p>like <a href="#">KLT</a>,<a href="#">SFM/SLAM</a>,
                depth/pose/EgoMotion estimation
            </p>
        </div>
            
        <a href="#" class="more btn" id="affine">What is affine transform?</a>
        <div class='affine'>
            <h2>Translation</h2>
            <p>A regular translation of an image can be found by applying the below transform on the input image</p>
            
            <div lang='latex' style="text-align:center">
                    \begin{bmatrix}X_1+p_{1}  \\ Y_1 + p_{2} \end{bmatrix}
                    =
                    \begin{bmatrix}X_2 \\ Y_2 \end{bmatrix}
                </div>
                
                <p>This will translate the image by $p_{1}$ along the $x$ and $p_{2}$ along the $y$ axis. (0, 0)-->(p1, p2).</p>
                
                
                <h2>Affine</h2>
                <p>Affine transformation includes the effects of scaling, translating and rotating the image.</p>
                <div lang='latex' style="text-align:center">
                        \begin{bmatrix}1+p_{1} & p_{3} & p_5 \\p_2 & 1+p_4 & p_6 \end{bmatrix}
                        * \begin{bmatrix}X_1 \\ Y_1 \\ 1 \end{bmatrix}
                        =
                        \begin{bmatrix}(1+p_1)*X_1 + p_3*Y_1 + p_5 \\ p_2*X_1 + (1+p_4)*Y_1 + p_6 \end{bmatrix}
                        =
                        \begin{bmatrix}X_2 \\ Y_2 \end{bmatrix}
                </div>
                <div style="width:image width px; font-size:90%; text-align:center">
                        <img src = "/img/research/transforms.png" style="width:40%; height:80%;" alt = 'alt text'/>
                        <br/>
                        Affine examples.
                        Figure 2
                </div>
        </div>
                        
                        
        <a href="#" class="more btn" id="aim">How do we track?</a>
        <div class='aim'>
            <h2>Intuition</h2>
            <p>Consider two cropped images bounding the object we wish to track</p>
            <ul>
                <li>The original image or the image given at t=n.</li>
                <li>The template image or the image given at t=n+1 which we wish to find.</li>
            </ul>
            <div style="width:image width px; font-size:90%; text-align:center">
                    <img src = "/img/research/img0092c.png" style="width:10%; height:10%;"  alt = 'alt text'/>
                    <img src = "/img/research/img0093c.png" style="width:10%; height:10%;" alt = 'alt text'/>
                    <br/>
                    Original image I and template image T.
                    <br/>
                    Figure 1
            </div>
            <strong>The template image can be thought of as an affine transformed version of our original image.</strong>
            <p>If we manage to find the exact parameters of the affine matrix for each pair of subsequent images we can
                serially track the object, given we have the ground truth of the object at t=0.</p>
    
        </div>
                
                        
        <a href="#" class="more btn" id="klt">Kanade Lucas Tomasi KLT tracker</a>
        <div class='klt'>
            <h2>Introduction and intuition</h2>
            <p>KLT tracker finds out the parameters $p$ of the affine matrix given the original image I and the template image T.</p>
            <p>We consider the initial paramters $p$ of the affine matrix to be all zeros (which suggests a unit affine transformation or $I$ == $T$)</p>
            <p>We then update the $p$ values by $\triangledown p$ iteratively such that the error between the warped original image $I(W(x;p))$ and
               the template image T is minimised where $W(x;p)$ is the affine transform applied.
            </p>
            <p>Generally we keep iterating until the norm of $\triangledown p \leq \alpha $ where $\alpha$ is our error tolerance.</p>
        </div>

        <a href="#" class="more btn" id="klt_deriv">KLT mathematical derivation</a>
        <div class='klt_deriv'>
            <p>The goal is to minimise the mean square error between the warped original image $I(W(x;p))$ and the template image $T$. </p> 
            <div lang='latex' style="text-align:center">
                \sum_{x} \left[I(W(x;p + \triangle p)) - T(x)\right]^2
            </div>
            <p>First order taylor expansion </p> 
            <div lang='latex' style="text-align:center">
                \sum_{x} \left[I(W(x;p)) + \triangledown I \frac{\partial W}{\partial p} \triangle p - T(x)\right]^2
            </div>
            <p>The error is minimised by making the first order derivative of the error as zero. </p> 
            <div lang='latex' style="text-align:center">
                \sum_{x} \left[ \triangledown I \frac{\partial W}{\partial p} \right]^T \left[I(W(x;p)) + \triangledown I \frac{\partial W}{\partial p} \triangle p - T(x)\right] = 0
            </div>
            <p>Reshuffling .. </p> 
            <div lang='latex' style="text-align:center">
                \triangle p = H^{-1}\sum_{x} \left[ \triangledown I \frac{\partial W}{\partial p} \right]^T \left[ T(x) - I(W(x;p))\right]
            </div>
            <p> Where</p>
            <div lang='latex' style="text-align:center">
                H = \left[\triangledown I \frac{\partial W}{\partial p} \right]^T \left[\triangledown I \frac{\partial W}{\partial p} \right]
            </div>
            <p> Pictorially - </p>
            <div style="width:image width px; font-size:90%; text-align:center">
                <img src = "/img/research/klt.png" alt = 'alt text'/>
                <br/>
            </div>    
        </div>

        <a href="#" class="more btn" id="klt_assumpt">Assumptions and Drawbacks of KLT</a>
        <div class='klt_assumpt'>
            <h2>Assumptions</h2>
            <ul>
                <li>
                    Brightness constancy - KLT assumes that the intensity of the pixels of the object which is being tracked will correspondigly be consistent in every subsequent image.  
                </li>
                <div style="width:image width px; font-size:90%; text-align:center">
                    <img src = "/img/research/intensity.png" alt = 'alt text'/>
                    <br/>
                </div>    
                <li>
                    Spatial coherence - The neighbouring points typically belong to the same surface and have similar motions.
                </li>
                <div style="width:image width px; font-size:90%; text-align:center">
                    <img src = "/img/research/spatial.png" alt = 'alt text'/>
                    <br/>
                </div>    
                <li>
                    Temporal persistence - Since the core of KLT lies in calculating the gradient of the image, it assumes that the pixel shift will be miniscule in subsequent images.
                </li>
                <div style="width:image width px; font-size:90%; text-align:center">
                    <img src = "/img/research/gradual.png" alt = 'alt text'/>
                    <br/>
                </div>    
            </ul>
            <h2>Drawbacks</h2>
            <p>While KLT is a pure geometry based approach, it has some inherent problems.</p>
            <ul>
                <li>
                    It relies too much on gradients - the heart of KLT lies in using the gradient of the image to calculate the update in affine matrix parameters.
                </li>
                <li>
                    At time this gradient can lose its reliablity. We want the walls (static) to have a higher gradient and a person (non-static) to have a lower gradient.
                </li>
            </ul>

        </div>

        <a href="#" class="more btn" id="klt_contri">Proposed solution(s)</a>
        <div class='klt_contri'>
            <p>With the advent of deep learning we can use neural networks to our aid.</p>
            <ul>
                <li>
                    We can use neural networks at appropriate position in KLT, possibly right after calculating the gradient of the image so as to extract important features
                    (which then act as a proxy for image gradients in KLT) that can account for the reliablity of the gradient and make the system more robust.
                </li>
                <li>
                    There are techniques already present which make use of the various flavour of neural networks for object tracking, but these techniques are known to
                    fail at instances because they go haywire in unknown environments (the environments which were not present in the training data.) 
                </li>
                <li>
                    Our proposed solution aims to capture the best of both worlds making a more reliable and robust object tracking system.
                </li>
            </ul>
            <div style="width:image width px; font-size:90%; text-align:center">
                <img src = "/img/research/neural.jpg" style="width:40%; height:60%;"  alt = 'alt text'/>
                <br/>
            </div>    
        </div>

        <a href="#" class="more btn" id="refer">References</a>
        <div class='refer'>
            <ul>
                <li>
                    <a href="http://www.ncorr.com/download/publications/bakerunify.pdf">Lucas-Kanade 20 Years On: A Unifying Framework</a>
                </li>
                <li>
                    <a href="http://web.yonsei.ac.kr/hgjung/Lectures/AUE859/8.%20Optical%20Flow,%20KLT%20feature%20tracker.pdf">Optical Flow,
                        KLT Feature Tracker - HANYANG University</a>
                </li>
                <li>
                    <a href="https://ags.cs.uni-kl.de/fileadmin/inf_ags/opt-ss15/OPT_SS2015_lec06.pdf">Computer Vision
                        Object and People Tracking - Kaiserlautern University</a>
                </li>
                <li>
                    <a href="http://lxu.me/projects/deepeaf/">Deep Edge-Aware Filters - <strong>Image credits</strong> </a>
                </li>
                <li>
                    <a href="https://www.ri.cmu.edu/pub_files/pub3/chang_peng_2003_1/chang_peng_2003_1.pdf">ROBUST TRACKING AND STRUCTURE FROM MOTION WITH
                        SAMPLING METHOD</a>
                </li>
                <li>
                    <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">Simultaneous localization and mapping</a>
                </li>
                
            </ul>
        </div>

    <script src="/js/toggle_button.js"></script>
    </body>
</html>